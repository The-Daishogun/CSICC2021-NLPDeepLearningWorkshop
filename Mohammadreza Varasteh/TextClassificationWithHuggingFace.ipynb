{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Text Classification with huggingface.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "KaNRFtTHL4fH",
        "EUJreV2DX71a",
        "VGdyN6jIYPni",
        "1agsCPwzYjcR",
        "iX4MQGmIgSRA",
        "MVKcrSYZ168t",
        "1jR7OI_sdTpC",
        "bKg5DV8fdasI",
        "71Zv606XMXhe",
        "3LLOwTR77pYz",
        "AA97Jz4S72uY",
        "1MZFMQkPMxcN",
        "puTe5s9GM68a",
        "rj6n5nlVNBIi",
        "eqdgspJONMiT",
        "EkZbsQfKNRs2"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaNRFtTHL4fH"
      },
      "source": [
        "# چک کردن کارت گرافیک"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6zCWaUiY14P"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUJreV2DX71a"
      },
      "source": [
        "#دیتاست\n",
        "\n",
        "پیکره خبری فارسی پرسیکا جمع آوری شده توسط دکتر اقبال زاده و همکاران\n",
        "\n",
        "[لینک دانلود](https://sourceforge.net/projects/persica)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcUpuR8wVaEy"
      },
      "source": [
        "!wget \"https://sourceforge.net/projects/persica/files/persica.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGdyN6jIYPni"
      },
      "source": [
        "# نصب کردن کتابخانه هضم برای تمیز کردن پیکره\n",
        "[گیت‌هاب](https://github.com/sobhe/hazm) \n",
        "\n",
        "[وب‌سایت](https://www.sobhe.ir/hazm/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbMXVoHKWIvh"
      },
      "source": [
        "!pip install -q hazm "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxvOdpS2YeWx"
      },
      "source": [
        "# پیکره"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0SmGMe8WnZp"
      },
      "source": [
        "import pandas as pd\n",
        "from hazm.PersicaReader import PersicaReader\n",
        "\n",
        "data = {\n",
        "    \"id\": [],\n",
        "    \"title\": [],\n",
        "    \"text\": [],\n",
        "    \"date\": [],\n",
        "    \"time\": [],\n",
        "    \"category\": [],\n",
        "    \"category2\": []\n",
        "}\n",
        "\n",
        "for entry in PersicaReader('/content/persica.csv').docs():\n",
        "\n",
        "    data['id'].append(entry['id'])\n",
        "    data['title'].append(entry['title'])\n",
        "    data['text'].append(entry['text'])\n",
        "    data['date'].append(entry['date'])\n",
        "    data['time'].append(entry['time'])\n",
        "    data['category'].append(entry['category'])\n",
        "    data['category2'].append(entry['category2'])\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1agsCPwzYjcR"
      },
      "source": [
        "## حذف کردن قسمت های اضافه"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHzTFiQTXoBB"
      },
      "source": [
        "df = df.replace(\"\", float(\"NaN\"))\n",
        "df = df.dropna().reset_index(drop=True)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5V3zM8FYmyE"
      },
      "source": [
        "df = df[['title','text','category2']]\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iX4MQGmIgSRA"
      },
      "source": [
        "## تبدیل کردن دسته بندی های فارسی به انگلیسی"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtgL6aECgYAz"
      },
      "source": [
        "translation_dictionary = {\n",
        "    \"مذهبي\" : \"Religion\",\n",
        "    \"آموزشي\": \"Education\",\n",
        "    \"فقه و حقوق\": \"Law\",\n",
        "    \"بهداشتي\": \"Health\",\n",
        "    \"علمي\": \"Science\",\n",
        "    \"تاريخي\": \"History\",\n",
        "    \"ورزشي\": \"Sports\",\n",
        "    \"اقتصادي\": \"Economy\",\n",
        "    \"فرهنگي\": \"Culture\",\n",
        "    \"سياسي\": \"Politics\",\n",
        "    \"اجتماعي\": \"Social\"\n",
        "}\n",
        "df[\"en_category\"] = df.category2.apply(lambda x: translation_dictionary[x])\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVKcrSYZ168t"
      },
      "source": [
        "## بررسی توزیع ورودی های هر کلاس"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zK2dMnzJZCII"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "counts = df.en_category.value_counts()\n",
        "\n",
        "labels = list(counts.keys())\n",
        "sizes = list(counts.values)\n",
        "colors = ['#ff9999','#66b3ff','#99ff99','#ffcc99']\n",
        "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', shadow=True, startangle=140)\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lL9tnRHWZKrR"
      },
      "source": [
        "## پیکره"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jR7OI_sdTpC"
      },
      "source": [
        "### تعریف یک تابع برای تمیز کردن پیکره"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Xk4wAkJZR4h"
      },
      "source": [
        "import hazm\n",
        "normalizer = hazm.Normalizer(persian_numbers=True)\n",
        "tokenizer = hazm.WordTokenizer()\n",
        "stopwords = hazm.stopwords_list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnMY7QDsdNgL"
      },
      "source": [
        "def clean_text(sentence):\n",
        "    sentence = normalizer.normalize(sentence)\n",
        "    sentence = tokenizer.tokenize(sentence)\n",
        "    return \" \".join([word for word in sentence if word not in stopwords])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKg5DV8fdasI"
      },
      "source": [
        "### استفاده از تابع بالا"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NxPBofmdeMg"
      },
      "source": [
        "df = df.dropna()\n",
        "df['cleaned_text'] = df[\"title\"] + \" \" + df['text']\n",
        "df['cleaned_text'] = df.cleaned_text.apply(clean_text)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71Zv606XMXhe"
      },
      "source": [
        "### اختصاص دادن یک عدد به هر کلاس"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cITP1qV8eKry"
      },
      "source": [
        "labels = set(df.en_category)\n",
        "print(f\"labels: {labels}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taICXg8Qfcc7"
      },
      "source": [
        "label2id = {label: i for i, label in enumerate(labels)}\n",
        "id2label = {i: label for label, i in label2id.items()}\n",
        "print(f'label2id: {label2id}')\n",
        "print(f'id2label: {id2label}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPWnJPptiAw0"
      },
      "source": [
        "df['num_category'] = df.en_category.apply(lambda x: label2id[x])\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Lc61aNwe5eV"
      },
      "source": [
        "df = df[['cleaned_text', 'num_category', 'en_category']]\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LLOwTR77pYz"
      },
      "source": [
        "### بُر زدن پیکره"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MyrydYBiYO3"
      },
      "source": [
        "df = df.sample(frac=1)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECs9MA_1ihrM"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(df, test_size=0.2, random_state=1)\n",
        "train, valid = train_test_split(train, test_size=0.2, random_state=1)\n",
        "\n",
        "train = train.reset_index(drop=True)\n",
        "valid = valid.reset_index(drop=True)\n",
        "test = test.reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Mde-2x0Vwmt"
      },
      "source": [
        "print(f\"training data: {train.shape}\")\n",
        "print(f\"validation data: {valid.shape}\")\n",
        "print(f\"test data: {test.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AA97Jz4S72uY"
      },
      "source": [
        "### تبدیل دیتافریم پانداس به دیتاست هاگینگ فیس"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIhG8kMu-RPN"
      },
      "source": [
        "!pip install -q datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqaXxG0bjDq8"
      },
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "train_dataset = Dataset.from_pandas(train)\n",
        "valid_dataset = Dataset.from_pandas(valid)\n",
        "test_dataset = Dataset.from_pandas(test)\n",
        "\n",
        "train_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MZFMQkPMxcN"
      },
      "source": [
        "### تعریف یک تابع جهت آماده سازی دیتا برای آموزش"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUS7VyPIkIhR"
      },
      "source": [
        "MAX_LEN = 128"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYK2rIV6SuIx"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def tensorify(dataset, tokenizer, max_len=MAX_LEN, tfds=True):\n",
        "    dataset = dataset.map(lambda e: tokenizer(e['cleaned_text'], padding=True, max_length=max_len), batched=True)\n",
        "    dataset.set_format(type='tensorflow', columns=['input_ids','token_type_ids', 'attention_mask','num_category'])\n",
        "    features = {x: dataset[x].to_tensor(default_value=0, shape=[None, max_len]) for x in ['input_ids', 'token_type_ids', 'attention_mask']}\n",
        "    if tfds:\n",
        "        tfdataset = tf.data.Dataset.from_tensor_slices((features, dataset[\"num_category\"]))\n",
        "        return tfdataset\n",
        "    else:\n",
        "        return features, dataset[\"num_category\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puTe5s9GM68a"
      },
      "source": [
        "### آماده کردن توکنایزر پارس برت و تبدیل پیکره به بردار\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fJPyQXN-VU8"
      },
      "source": [
        "!pip install -q transformers "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZesVx3jkU0ZJ"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "MODEL_NAME = 'HooshvareLab/bert-fa-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "train_ds = tensorify(train_dataset, tokenizer)\n",
        "valid_ds = tensorify(valid_dataset, tokenizer)\n",
        "test_ds = tensorify(test_dataset, tokenizer)\n",
        "xtest, ytest = tensorify(test_dataset, tokenizer, tfds=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rj6n5nlVNBIi"
      },
      "source": [
        "### محاصبه کردن قدم ها و آماده کردن دیتاست"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "eNExYDB-kyBd"
      },
      "source": [
        "TRAIN_BATCH_SIZE = 8\n",
        "VALID_BATCH_SIZE = 8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HfQ2cshqS7C-"
      },
      "source": [
        "train_steps = len(train) // TRAIN_BATCH_SIZE\n",
        "valid_steps = len(valid) // VALID_BATCH_SIZE\n",
        "\n",
        "train_steps, valid_steps"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DdXbDgM8k5ff"
      },
      "source": [
        "train_dataset = train_ds.batch(TRAIN_BATCH_SIZE).repeat()\n",
        "valid_dataset = valid_ds.batch(VALID_BATCH_SIZE).repeat()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ut_cfY--94Tn"
      },
      "source": [
        "# مدل"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqdgspJONMiT"
      },
      "source": [
        "### تنظیم مدل"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oI5cmAFhaR34"
      },
      "source": [
        "from transformers import BertConfig\n",
        "\n",
        "config = BertConfig.from_pretrained(MODEL_NAME, **{\n",
        "    'label2id': label2id,\n",
        "    'id2label': id2label\n",
        "})\n",
        "print(config.to_json_string())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkZbsQfKNRs2"
      },
      "source": [
        "### تعریف تابع برای آماده کردن مدل"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4xI6jhs7aUP-"
      },
      "source": [
        "from transformers import TFBertForSequenceClassification\n",
        "import tensorflow as tf\n",
        "\n",
        "def build_model():\n",
        "    model = TFBertForSequenceClassification.from_pretrained(MODEL_NAME, config=config)\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
        "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xB-W_GqyNV2w"
      },
      "source": [
        "### ساخت مدل و آموزش آن"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-4HHcZ3wlQRS"
      },
      "source": [
        "EPOCHS = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7VnzkEDaeo9"
      },
      "source": [
        "model = build_model()\n",
        "r = model.fit(\n",
        "            train_dataset,\n",
        "            validation_data=valid_dataset,\n",
        "            steps_per_epoch=train_steps,\n",
        "            validation_steps=valid_steps,\n",
        "            epochs=EPOCHS,\n",
        "            verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mYb9VBx8119"
      },
      "source": [
        "### بررسی روند آموزش"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvrDWRquanI9"
      },
      "source": [
        "acc = r.history['accuracy']\n",
        "val_acc = r.history['val_accuracy']\n",
        "loss = r.history['loss']\n",
        "val_loss = r.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'b', label='Training acc', color=\"red\")\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'b', label='Training loss', color=\"red\")\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgld-96LNdQo"
      },
      "source": [
        "## ارزیابی مدل"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJcDya1J8_bt"
      },
      "source": [
        "### Validation دقت"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HInbtl48phY5"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "final_accuracy = r.history['val_accuracy']\n",
        "print('FINAL ACCURACY MEAN: ', np.mean(final_accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xj25erlPNndh"
      },
      "source": [
        "### Test دقت"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7c_VakqliIh"
      },
      "source": [
        "TEST_BATCH_SIZE = 8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIs-jmNKawEs"
      },
      "source": [
        "ev = model.evaluate(test_ds.batch(TEST_BATCH_SIZE))\n",
        "print()\n",
        "print(f'Evaluation: {ev}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lzv7aSpNr9l"
      },
      "source": [
        "### پیش بینی بر روی دیتا های تست"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDJDltjIx0_L"
      },
      "source": [
        "predictions = model.predict(xtest)\n",
        "ypred = [ np.argmax(i) for i in predictions[0]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ME8aNvUCNw4P"
      },
      "source": [
        "### گزارش طبقه بندی"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ye-qkiWe_Vc1"
      },
      "source": [
        "from sklearn.metrics import classification_report, f1_score\n",
        "\n",
        "class_names = id2label.values()\n",
        "print(classification_report(ytest.numpy().tolist(), ypred, target_names=class_names))\n",
        "print(f'F1: {f1_score(ytest.numpy().tolist(), ypred, average=\"weighted\")}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npDQDvy_N0ez"
      },
      "source": [
        "## ذخیره کردن مدل"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "si99wUpiatgQ"
      },
      "source": [
        "import os\n",
        "\n",
        "OUTPUT_PATH = '/content/trained_model/parsbert_clf.bin'\n",
        "\n",
        "os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n",
        "\n",
        "model.save_pretrained(os.path.dirname(OUTPUT_PATH))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7X0110Y9bns"
      },
      "source": [
        "### لود کردن مدل ذخیره شده برای استفاده مجدد"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Cq-oWIEruDO"
      },
      "source": [
        "saved_model = TFBertForSequenceClassification.from_pretrained(\"/content/trained_model/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksBtIYrIHgpo"
      },
      "source": [
        "saved_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIRLs2Jx0aqb"
      },
      "source": [
        "# برای مطالعه بیشتر\n",
        "\n",
        "[وبسایت هاگینگ‌فیس](https://huggingface.co/)\n",
        "\n",
        "[داکیومنتیشن ترنسفرمرها](https://huggingface.co/transformers/)\n",
        "\n",
        "[داکیومنتیشن دیتاستز](https://huggingface.co/docs/datasets/)\n",
        "\n",
        "[داکیومنتیشن توکنایزر ها](https://huggingface.co/docs/tokenizers/python/latest/)\n",
        "\n",
        "[مقاله پارس‌برت](https://arxiv.org/abs/2005.12515)\n",
        "\n"
      ]
    }
  ]
}